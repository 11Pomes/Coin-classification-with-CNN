{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import imutils\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "import h5py\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "imageSize = 128\n",
    "\n",
    "def save():\n",
    "    roman_coins = []\n",
    "    files = os.listdir(\"dataset/\")\n",
    "    files = [file for file in files if file.endswith(\".png\")]\n",
    "    shuffle(files)\n",
    "    X = []\n",
    "    for i, file in enumerate(files):\n",
    "        if(file.startswith('class')):\n",
    "            roman_coins += [i]\n",
    "        print(\"Adding file {}/{}\".format(i, len(files)))\n",
    "        #img = cv2.imread(\"Rotated/\" + file, -1)\n",
    "        img = cv2.imread(\"dataset/\" + file, -1)\n",
    "\n",
    "        # Convert grayscale to RGB\n",
    "        if len(img.shape) == 2:\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "        # Resize to desired size and normalize\n",
    "        img = cv2.resize(img, (imageSize, imageSize))\n",
    "        img = img.astype(float) / 255.\n",
    "        X.append(img)\n",
    "\n",
    "    # Convert to numpy array\n",
    "    X = np.array(X)\n",
    "\n",
    "    # 10% for testing\n",
    "    testProportion = 0.1\n",
    "    trainNb = int(X.shape[0] * (1 - testProportion))\n",
    "    testNb = X.shape[0] - trainNb\n",
    "\n",
    "    print(\"Splitting dataset in X_train({}) and X_test({})...\".format(trainNb, testNb))\n",
    "    # Split train test\n",
    "    X_train = X[:trainNb]\n",
    "    X_test = X[-testNb:]\n",
    "\n",
    "    print(len(X_train))\n",
    "\n",
    "    # Save at hdf5 format\n",
    "    \"Saving dataset in hdf5 format... - this may take a while\"\n",
    "    h5f = h5py.File(\"coins.h5\", 'w')\n",
    "    h5f.create_dataset('X_train', data=X_train)\n",
    "    h5f.create_dataset('X_test', data=X_test)\n",
    "    h5f.create_dataset('roman_coins', data=roman_coins)\n",
    "    h5f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = Path(\"coins.h5\")\n",
    "if not file.is_file():\n",
    "    save()\n",
    "X_test = h5py.File('coins.h5','r')['X_test']\n",
    "X_train = h5py.File('coins.h5','r')['X_train']\n",
    "coins_index = h5py.File('coins.h5','r')['roman_coins']\n",
    "\n",
    "ntrain = 2366\n",
    "ntest  = 263\n",
    "dim    = 128*128*3\n",
    "nclass = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "roman_coins = []\n",
    "for i in coins_index:\n",
    "    roman_coins += [i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = []\n",
    "y_test = []\n",
    "test_limit = len(X_train)\n",
    "for i, data in enumerate(X_train):\n",
    "    if i in roman_coins:\n",
    "        y_train += [1]\n",
    "    else:\n",
    "        y_train += [0]\n",
    "\n",
    "for i, data in enumerate(X_test):\n",
    "    if i + test_limit in roman_coins:\n",
    "        y_test += [1]\n",
    "    else:\n",
    "        y_test += [0]\n",
    "\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_shape = X_train.shape\n",
    "x_train = np.ndarray(shape=X_train.shape, dtype=float)\n",
    "x_test = np.ndarray(shape=X_test.shape, dtype=float)\n",
    "for i, data in enumerate(X_train):\n",
    "    x_train[i] = data\n",
    "for i, data in enumerate(X_test):\n",
    "    x_test[i] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define the characteristics of the input image \n",
    "height = 128\n",
    "width = 128\n",
    "channels = 3\n",
    "n_inputs = height * width * 3\n",
    "\n",
    "# We define the parameters of the layers according to \n",
    "# description previously presented\n",
    "\n",
    "conv1_fmaps = 16       #32\n",
    "conv1_ksize = 3\n",
    "conv1_stride = 1\n",
    "conv1_pad = \"SAME\"\n",
    "\n",
    "conv2_fmaps = 24     #64\n",
    "conv2_ksize = 3\n",
    "conv2_stride = 2\n",
    "conv2_pad = \"SAME\"\n",
    "\n",
    "pool3_fmaps = conv2_fmaps\n",
    "\n",
    "n_fc1 = 64\n",
    "n_outputs = 2\n",
    "\n",
    "\n",
    "with tf.name_scope(\"inputs\"):\n",
    "    # Variable X is passed as a vector\n",
    "    X = tf.placeholder(tf.float32, shape=[None, n_inputs], name=\"X\")\n",
    "    # It is reshaped to the tensor according to image size an channels\n",
    "    X_reshaped = tf.reshape(X, shape=[-1, height, width, channels])\n",
    "    # Class of each MNIST image\n",
    "    y = tf.placeholder(tf.int32, shape=[None], name=\"y\")\n",
    "\n",
    "# The first layer is defined. Notice that the tensorflow function used\n",
    "# is tf.layers.conv2d(). Also the parameters are those previously defined.\n",
    "\n",
    "\n",
    "conv1 = tf.layers.conv2d(X_reshaped, filters=conv1_fmaps, kernel_size=conv1_ksize,\n",
    "                         strides=conv1_stride, padding=conv1_pad,\n",
    "                         activation=tf.nn.relu, name=\"conv1\")\n",
    "\n",
    "# The second layer is defined. Notice that the input of this layer is the output\n",
    "# of the previous layer. You can check that conv1 has size 28x28 and conv2 has\n",
    "# size 14x14 (Try to find out why)\n",
    "\n",
    "conv2 = tf.layers.conv2d(conv1, filters=conv2_fmaps, kernel_size=conv2_ksize,\n",
    "                         strides=conv2_stride, padding=conv2_pad,\n",
    "                         activation=tf.nn.relu, name=\"conv2\")\n",
    "\n",
    "# The maxpool layer is defined. Notice that the  tf.nn.max_pool() is used to define the layer.\n",
    "# Also, maxpool is applied to each of the conv2_fmaps filters, and since the inputs have size\n",
    "# 14x14, Stride=2 and Padding=Valid, after applying maxpool we have pool3_fmaps filters\n",
    "# of size (7x7). That is the reason while the output is reshaped (flattened) to (pool3_fmaps * 7 * 7)\n",
    "\n",
    "with tf.name_scope(\"pool3\"):\n",
    "    pool3 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"VALID\")\n",
    "    pool3_flat = tf.reshape(pool3, shape=[-1, pool3_fmaps * 32 * 32])\n",
    "\n",
    "# This is the full layer. From previous classes we already know function tf.layers.dense()\n",
    "# used to define full layers. \n",
    "# The number of input and output units is the same (pool3_fmaps * 7 * 7)\n",
    "with tf.name_scope(\"fc1\"):\n",
    "    fc1 = tf.layers.dense(pool3_flat, n_fc1, activation=tf.nn.relu, name=\"fc1\")\n",
    "\n",
    "# This is the output layer where the network produces a classification for each class\n",
    "# The classification is used using the function softmax that we have studied in the previous lab\n",
    "with tf.name_scope(\"output\"):\n",
    "    logits = tf.layers.dense(fc1, n_outputs, name=\"output\")\n",
    "    Y_proba = tf.nn.softmax(logits, name=\"Y_proba\")\n",
    "\n",
    "# The loss function and optimizers are defined as in previous labs.    \n",
    "with tf.name_scope(\"train\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y)\n",
    "    loss = tf.reduce_mean(xentropy)\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "# We define two functions to evaluate the quality of the network as classifier\n",
    "# Correct computes, for a batch of observations, how many were correctly classified.\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "# We define a saver \n",
    "with tf.name_scope(\"init_and_save\"):\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train accuracy: 0.93 Test accuracy: 0.2157\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 1\n",
    "IMAGE_SIZE = 128\n",
    "NUM_CHANNELS = 1\n",
    "BATCH_SIZE = 100\n",
    "test_size = 10000\n",
    "\n",
    "config = tf.ConfigProto(device_count = {'GPU': 0})\n",
    "with  tf.Session(config=config) as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(2366 // BATCH_SIZE):\n",
    "            #X_batch, y_batch = mnist.train.next_batch(BATCH_SIZE)\n",
    "            randidx = np.random.randint(ntrain, size=BATCH_SIZE)\n",
    "            X_batch = x_train[randidx, :]\n",
    "            y_batch = y_train[randidx]  \n",
    "            X_batch = np.reshape(X_batch, (-1, 128*128*3))\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "\n",
    "\n",
    "        correct_pred = 0\n",
    "        for iteration in range(2366 // BATCH_SIZE):\n",
    "            randidx = np.random.randint(ntrain, size=BATCH_SIZE)\n",
    "            X_batch = x_train[randidx, :]\n",
    "            y_batch = y_train[randidx]  \n",
    "            X_batch = np.reshape(X_batch, (-1, 128*128*3))\n",
    "            correct_pred += np.sum(correct.eval(feed_dict={X: X_batch, y: y_batch}))\n",
    "            #print(correct_pred)\n",
    "            \n",
    "        acc_test = (correct_pred / float(test_size))\n",
    "        #acc_test = accuracy.eval(feed_dict={X: mnist.test.images, y: mnist.test.labels})\n",
    "        print(epoch, \"Train accuracy:\", acc_train, \"Test accuracy:\", acc_test)\n",
    "\n",
    "        #save_path = saver.save(sess, \"./my_mnist_model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
