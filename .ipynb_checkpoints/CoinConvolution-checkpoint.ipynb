{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coin Classification using Convolutional Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import imutils\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "import h5py\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset creation.\n",
    "\n",
    "Functions flip, rotate are for expanding coins dataset with modifying coins image by flipping and rotating. Folders Flipped, Rotated should be __existing in the same file system with this project__.\n",
    "\n",
    "Functions names and short explanation: \n",
    "- Images : 2629 coins images. 180 of 2629 are coins from Roman times.\n",
    "- Flipped: 10516 with flipped coins images. - For each coin image add 4 images of flipped coin versions.\n",
    "- Rotated: 21032 with rotated coins images. - For each coin image add 8 images of rotated coin versions.\n",
    "\n",
    "Dataset consist of 34177 images number representation which is fallowing this order: 2629(Base coin images) + 10516(Flipped coins images) + 21032(Rotated coins images).\n",
    "\n",
    "Images which starts with substring __'class'__ are classified as a Roman coins.\n",
    "\n",
    "All images are resized to 28x28 pixels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "imageSize = 28\n",
    "roman_index = []\n",
    "\n",
    "def flip():\n",
    "    global roman_index\n",
    "    files = [file for file in os.listdir(\"Images\") if file.endswith(\".png\") or file.endswith(\".jpg\")]\n",
    "    \n",
    "    for i, file in enumerate(files):\n",
    "        print(\"Flipping file {}/{}\".format(i,len(files)))\n",
    "        \n",
    "        if(file.startswith('class')):\n",
    "            roman_index += [i]\n",
    "\n",
    "        img = cv2.imread(\"Images/\"+file,-1)\n",
    "        rflip = cv2.flip(img,1)\n",
    "        vflip = cv2.flip(img,0)\n",
    "        rvflip = cv2.flip(rflip,0)\n",
    "\n",
    "        cv2.imwrite(\"Flipped/\"+file[:-4]+\"_.png\",img)\n",
    "        cv2.imwrite(\"Flipped/\"+file[:-4]+\"_r.png\",rflip)\n",
    "        cv2.imwrite(\"Flipped/\"+file[:-4]+\"_v.png\",vflip)\n",
    "        cv2.imwrite(\"Flipped/\"+file[:-4]+\"_rv.png\",rvflip)\n",
    "# Generates 2 rotations of the images\n",
    "def rotate():\n",
    "    global roman_index\n",
    "    files = [file for file in os.listdir(\"Flipped\") if file.endswith(\".png\") or file.endswith(\".jpg\")]\n",
    "\n",
    "    # 2 rotations * 4 flips = 8 possibilites\n",
    "    for index, file in enumerate(files):\n",
    "        print(\"Rotating file {}/{}\".format(index,len(files)))  \n",
    "        if(file.startswith('class')):\n",
    "            roman_index += [index]\n",
    "        img = cv2.imread(\"Flipped/\"+file,-1)\n",
    "        cv2.imwrite(\"Rotated/\"+file[:-4]+\"_0.png\",img)\n",
    "        img = np.rot90(img)\n",
    "        cv2.imwrite(\"Rotated/\"+file[:-4]+\"_90.png\",img)\n",
    "\n",
    "# Save h5 dataset\n",
    "def save():\n",
    "    global roman_index\n",
    "    #Load, normalize and reshape\n",
    "    files = os.listdir(\"Rotated/\")\n",
    "    files = [file for file in files if file.endswith(\".png\")]\n",
    "\n",
    "    shuffle(files)\n",
    "    X = []\n",
    "    for i, file in enumerate(files):\n",
    "        print(\"Adding file {}/{}\".format(i,len(files)))\n",
    "        \n",
    "        if(file.startswith('class')):\n",
    "            roman_index += [i]\n",
    "            \n",
    "        img = cv2.imread(\"Rotated/\"+file, -1)\n",
    "\n",
    "        # Convert grayscale to RGB\n",
    "        if len(img.shape) == 2:\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "        # Resize to desired size and normalize\n",
    "        img = cv2.resize(img,(imageSize,imageSize))\n",
    "        img = img.astype(float)/255.\n",
    "        X.append(img)\n",
    "\n",
    "    # Convert to numpy array\n",
    "    X = np.array(X)\n",
    "\n",
    "    # 10% for testing\n",
    "    testProportion = 0.1\n",
    "    trainNb = int(X.shape[0]*(1-testProportion))\n",
    "    testNb = X.shape[0] - trainNb\n",
    "\n",
    "    print(\"Splitting dataset in X_train({}) and X_test({})...\".format(trainNb,testNb))\n",
    "    # Split train test\n",
    "    X_train = X[:trainNb]\n",
    "    X_test = X[-testNb:]\n",
    "\n",
    "    #Save at hdf5 format\n",
    "    print(\"Saving dataset in hdf5 format... - this may take a while\")\n",
    "    h5f = h5py.File(\"coins.h5\", 'w')\n",
    "    h5f.create_dataset('X_train', data=X_train)\n",
    "    h5f.create_dataset('X_test', data=X_test)\n",
    "    h5f.create_dataset('roman_coins', data=roman_index)\n",
    "    h5f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset is saved as h5 extension. \n",
    "Full generated dataset name: \"coins.h5\".\n",
    "\n",
    "If coins.h5 file not exist in the current project file system, functions for dataset generation(flip, rotate and save) are executed. Otherwise all required components are extracted from generated dataset.\n",
    "\n",
    "### Required components:\n",
    "- X_train : Coins images representation in RGB(Red, Green, Blue) number notation for each image pixel - used for training Convolution Network.\n",
    "- X_test : Coins images representation in RGB(Red, Green, Blue) number notation for each image pixel - used for testing Convolution Network performance.\n",
    "- roman_coins: A list of indexes of Roman coins - used for classification of Roman and non Roman coins.\n",
    "\n",
    "Training and testing data was splitted into two parts: Training data consist of 18928 units of coins representation data and testing data has 2104 units of coins representation data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = Path(\"coins.h5\")\n",
    "if not file.is_file():\n",
    "    print(\"This script creates 21208k images from the base 2652\")\n",
    "    print (\"Make sure the folders Rotated and Flipped exist\")\n",
    "    flip()\n",
    "    rotate()\n",
    "    save()\n",
    "X_train = h5py.File('coins.h5','r')['X_train']\n",
    "X_test = h5py.File('coins.h5','r')['X_test']\n",
    "coins_index = h5py.File('coins.h5','r')['roman_coins']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing.\n",
    "\n",
    "Since features training and testing data is already available from h5 dataset, it is necesarry convert data to Python friendly data structure - numpy array list. \n",
    "<br>\n",
    "For classifcation part, it is needed to extract values from h5 dataset with indexes of Roman coins and preprocess this data for all available coins. \n",
    "\n",
    "### Classification\n",
    "A new list witch contains an index of Roman coins is created to extract and save values from h5 dataset to a python based structure. When this step is done, a labels variables y_train and y_test is created which stores each picture classification value:\n",
    "- 0 : Representation of non Roman coin.\n",
    "- 1 : Representation of Roman coin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "roman_coins = []\n",
    "for i in coins_index:\n",
    "    roman_coins += [i]\n",
    "\n",
    "y_train = []\n",
    "y_test = []\n",
    "test_limit = len(X_train)\n",
    "for i, data in enumerate(X_train):\n",
    "    if i in roman_coins:\n",
    "        y_train += [1]\n",
    "    else:\n",
    "        y_train += [0]\n",
    "\n",
    "for i, data in enumerate(X_test):\n",
    "    if i + test_limit in roman_coins:\n",
    "        y_test += [1]\n",
    "    else:\n",
    "        y_test += [0]\n",
    "\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features extraction from h5 dataset\n",
    "All images are traversed and stored to the Python data structure for Convolution Network processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train = np.ndarray(shape=X_train.shape, dtype=float)\n",
    "x_test = np.ndarray(shape=X_test.shape, dtype=float)\n",
    "for i, data in enumerate(X_train):\n",
    "    x_train[i] = data\n",
    "for i, data in enumerate(X_test):\n",
    "    x_test[i] = data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution Network\n",
    "Parameters short explanation for Convolution Network:\n",
    "- __Height and Width__ - Image height and weight. Our images are made of 28*28 pixels, so as a height and width numbers are the same.\n",
    "- __Channels__ - Represents image color set. In our project images consist of combination of three colors: red, green, blue(RGB). In this case channels value is 3.\n",
    "- __n_inputs__ - Number of inputs which are calculated by multiplying height, width and channel number. In our project inputs value is 2352(28*28*3).\n",
    "- __ntrain__ - Number of all instances of training set (18928).\n",
    "- __ntest__ - Number of all instances of testing set (2104).\n",
    "- __conv1_fmaps, conv2_fmaps__ - Feature map for the first(conv1_fmaps) and second(conv2_fmaps) layers. The feature map is the output of one filter applied to the previous layer. A given filter is drawn across the entire previous layer, moved one pixel at a time. Each position results in an activation of the neuron and the output is collected in the feature map.\n",
    "- __conv1_ksize, conv2_ksize__ - Kernel size for first and second layers. Kernels are used for feature detection.\n",
    "- __conv1_stride, conv2_stride__ - The amount of shifts.\n",
    "- __conv1_pad, conv2_pad__ - Padding parameters. Parameter value is equal 'Same' which is Trying to pad evenly left and right, but if the amount of columns to be added is odd, it will add the extra column to the right\n",
    "- __pool3_fmaps__ - Pooling feature map.\n",
    "- __n_fc1__ - Fully connected layer. Fully connected layer looks at what high level features most strongly correlate to a particular class and has particular weights so that when you compute the products between the weights and the previous layer, you get the correct probabilities for the different classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2352\n"
     ]
    }
   ],
   "source": [
    "print(28*28*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Variable conv1/kernel already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"<ipython-input-7-fc19c83a3cb6>\", line 43, in <module>\n    activation=tf.nn.relu, name=\"conv1\")\n  File \"/home/domantas/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/home/domantas/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-0cc9feacaa8a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m conv1 = tf.layers.conv2d(X_reshaped, filters=conv1_fmaps, kernel_size=conv1_ksize,\n\u001b[1;32m     41\u001b[0m                          \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconv1_stride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconv1_pad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m                          activation=tf.nn.relu, name=\"conv1\")\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;31m# The second layer is defined. Notice that the input of this layer is the output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/layers/convolutional.py\u001b[0m in \u001b[0;36mconv2d\u001b[0;34m(inputs, filters, kernel_size, strides, padding, data_format, dilation_rate, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, trainable, name, reuse)\u001b[0m\n\u001b[1;32m    606\u001b[0m       \u001b[0m_reuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreuse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m       _scope=name)\n\u001b[0;32m--> 608\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/layers/base.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    669\u001b[0m       \u001b[0mOutput\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m     \"\"\"\n\u001b[0;32m--> 671\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m   def _add_inbound_node(self,\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/layers/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    557\u001b[0m           \u001b[0minput_shapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 559\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    560\u001b[0m           \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/layers/convolutional.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    141\u001b[0m                                     \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_constraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m                                     \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m                                     dtype=self.dtype)\n\u001b[0m\u001b[1;32m    144\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_bias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m       self.bias = self.add_variable(name='bias',\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/layers/base.py\u001b[0m in \u001b[0;36madd_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, trainable, constraint)\u001b[0m\n\u001b[1;32m    456\u001b[0m                                    \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m                                    \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m                                    trainable=trainable and self.trainable)\n\u001b[0m\u001b[1;32m    459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvariable\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexisting_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mvariable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[1;32m   1201\u001b[0m       \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m       \u001b[0muse_resource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_getter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1203\u001b[0;31m       constraint=constraint)\n\u001b[0m\u001b[1;32m   1204\u001b[0m get_variable_or_local_docstring = (\n\u001b[1;32m   1205\u001b[0m     \"\"\"%s\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[1;32m   1090\u001b[0m           \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1091\u001b[0m           \u001b[0muse_resource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_getter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1092\u001b[0;31m           constraint=constraint)\n\u001b[0m\u001b[1;32m   1093\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1094\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[1;32m    423\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m           \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_resource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m           constraint=constraint)\n\u001b[0m\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m   def _get_partitioned_variable(\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[0;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint)\u001b[0m\n\u001b[1;32m    392\u001b[0m           \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m           use_resource=use_resource, constraint=constraint)\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcustom_getter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint)\u001b[0m\n\u001b[1;32m    740\u001b[0m                          \u001b[0;34m\"reuse=tf.AUTO_REUSE in VarScope? \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m                          \"Originally defined at:\\n\\n%s\" % (\n\u001b[0;32m--> 742\u001b[0;31m                              name, \"\".join(traceback.format_list(tb))))\n\u001b[0m\u001b[1;32m    743\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    744\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Variable conv1/kernel already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"<ipython-input-7-fc19c83a3cb6>\", line 43, in <module>\n    activation=tf.nn.relu, name=\"conv1\")\n  File \"/home/domantas/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/home/domantas/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n"
     ]
    }
   ],
   "source": [
    "# We define the characteristics of the input image \n",
    "height = 28\n",
    "width = 28\n",
    "channels = 3\n",
    "n_inputs = height * width * 3\n",
    "ntrain = len(X_train)\n",
    "ntest  = len(X_test)\n",
    "\n",
    "# We define the parameters of the layers according to \n",
    "# description previously presented\n",
    "\n",
    "conv1_fmaps = 16       #32\n",
    "conv1_ksize = 3\n",
    "conv1_stride = 1\n",
    "conv1_pad = \"SAME\"\n",
    "\n",
    "conv2_fmaps = 24     #64\n",
    "conv2_ksize = 3\n",
    "conv2_stride = 2\n",
    "conv2_pad = \"SAME\"\n",
    "\n",
    "pool3_fmaps = conv2_fmaps\n",
    "\n",
    "n_fc1 = 64\n",
    "n_outputs = 2\n",
    "\n",
    "\n",
    "with tf.name_scope(\"inputs\"):\n",
    "    # Variable X is passed as a vector\n",
    "    X = tf.placeholder(tf.float32, shape=[None, n_inputs], name=\"X\")\n",
    "    # It is reshaped to the tensor according to image size an channels\n",
    "    X_reshaped = tf.reshape(X, shape=[-1, height, width, channels])\n",
    "    # Class of each MNIST image\n",
    "    y = tf.placeholder(tf.int32, shape=[None], name=\"y\")\n",
    "\n",
    "# The first layer is defined. Notice that the tensorflow function used\n",
    "# is tf.layers.conv2d(). Also the parameters are those previously defined.\n",
    "\n",
    "\n",
    "conv1 = tf.layers.conv2d(X_reshaped, filters=conv1_fmaps, kernel_size=conv1_ksize,\n",
    "                         strides=conv1_stride, padding=conv1_pad,\n",
    "                         activation=tf.nn.relu, name=\"conv1\")\n",
    "\n",
    "# The second layer is defined. Notice that the input of this layer is the output\n",
    "# of the previous layer. You can check that conv1 has size 28x28 and conv2 has\n",
    "# size 14x14 (Try to find out why)\n",
    "\n",
    "conv2 = tf.layers.conv2d(conv1, filters=conv2_fmaps, kernel_size=conv2_ksize,\n",
    "                         strides=conv2_stride, padding=conv2_pad,\n",
    "                         activation=tf.nn.relu, name=\"conv2\")\n",
    "\n",
    "# The maxpool layer is defined. Notice that the  tf.nn.max_pool() is used to define the layer.\n",
    "# Also, maxpool is applied to each of the conv2_fmaps filters, and since the inputs have size\n",
    "# 14x14, Stride=2 and Padding=Valid, after applying maxpool we have pool3_fmaps filters\n",
    "# of size (7x7). That is the reason while the output is reshaped (flattened) to (pool3_fmaps * 7 * 7)\n",
    "\n",
    "with tf.name_scope(\"pool3\"):\n",
    "    pool3 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"VALID\")\n",
    "    pool3_flat = tf.reshape(pool3, shape=[-1, pool3_fmaps * 7 * 7])\n",
    "\n",
    "# This is the full layer. From previous classes we already know function tf.layers.dense()\n",
    "# used to define full layers. \n",
    "# The number of input and output units is the same (pool3_fmaps * 7 * 7)\n",
    "with tf.name_scope(\"fc1\"):\n",
    "    fc1 = tf.layers.dense(pool3_flat, n_fc1, activation=tf.nn.relu, name=\"fc1\")\n",
    "\n",
    "# This is the output layer where the network produces a classification for each class\n",
    "# The classification is used using the function softmax that we have studied in the previous lab\n",
    "with tf.name_scope(\"output\"):\n",
    "    logits = tf.layers.dense(fc1, n_outputs, name=\"output\")\n",
    "    Y_proba = tf.nn.softmax(logits, name=\"Y_proba\")\n",
    "\n",
    "# The loss function and optimizers are defined as in previous labs.    \n",
    "with tf.name_scope(\"train\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y)\n",
    "    loss = tf.reduce_mean(xentropy)\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "# We define two functions to evaluate the quality of the network as classifier\n",
    "# Correct computes, for a batch of observations, how many were correctly classified.\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "# We define a saver \n",
    "with tf.name_scope(\"init_and_save\"):\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train accuracy: 0.87 Test accuracy: 0.87\n",
      "1 Train accuracy: 0.88 Test accuracy: 0.86\n",
      "2 Train accuracy: 0.93 Test accuracy: 0.88\n",
      "3 Train accuracy: 0.89 Test accuracy: 0.86\n",
      "4 Train accuracy: 0.86 Test accuracy: 0.91\n",
      "5 Train accuracy: 0.9 Test accuracy: 0.92\n",
      "6 Train accuracy: 0.91 Test accuracy: 0.91\n",
      "7 Train accuracy: 0.93 Test accuracy: 0.93\n",
      "8 Train accuracy: 0.94 Test accuracy: 0.94\n",
      "9 Train accuracy: 0.92 Test accuracy: 0.88\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "IMAGE_SIZE = 28\n",
    "NUM_CHANNELS = 3\n",
    "BATCH_SIZE = 100\n",
    "test_size = ntest\n",
    "\n",
    "config = tf.ConfigProto(device_count = {'GPU': 0})\n",
    "with  tf.Session(config=config) as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(2366 // BATCH_SIZE):\n",
    "            randidx = np.random.randint(ntrain, size=BATCH_SIZE)\n",
    "            X_batch = x_train[randidx, :]\n",
    "            y_batch = y_train[randidx]  \n",
    "            X_batch = np.reshape(X_batch, (-1, n_inputs))\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "\n",
    "\n",
    "        correct_pred = 0\n",
    "        for iteration in range(ntrain // BATCH_SIZE):\n",
    "            randidx = np.random.randint(ntrain, size=BATCH_SIZE)\n",
    "            X_batch = x_train[randidx, :]\n",
    "            y_batch = y_train[randidx]  \n",
    "            X_batch = np.reshape(X_batch, (-1, n_inputs))\n",
    "            correct_pred += np.sum(correct.eval(feed_dict={X: X_batch, y: y_batch}))\n",
    "            #print(correct_pred)\n",
    "            \n",
    "#         acc_test = (correct_pred / float(test_size))\n",
    "        acc_test = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        print(epoch, \"Train accuracy:\", acc_train, \"Test accuracy:\", acc_test)\n",
    "\n",
    "        #save_path = saver.save(sess, \"./my_mnist_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
