{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import imutils\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "import h5py\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "imageSize = 128\n",
    "\n",
    "def save():\n",
    "    roman_coins = []\n",
    "    files = os.listdir(\"dataset/\")\n",
    "    files = [file for file in files if file.endswith(\".png\")]\n",
    "    shuffle(files)\n",
    "    X = []\n",
    "    for i, file in enumerate(files):\n",
    "        if(file.startswith('class')):\n",
    "            roman_coins += [i]\n",
    "        print(\"Adding file {}/{}\".format(i, len(files)))\n",
    "        #img = cv2.imread(\"Rotated/\" + file, -1)\n",
    "        img = cv2.imread(\"dataset/\" + file, -1)\n",
    "\n",
    "        # Convert grayscale to RGB\n",
    "        if len(img.shape) == 2:\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "        # Resize to desired size and normalize\n",
    "        img = cv2.resize(img, (imageSize, imageSize))\n",
    "        img = img.astype(float) / 255.\n",
    "        X.append(img)\n",
    "\n",
    "    # Convert to numpy array\n",
    "    X = np.array(X)\n",
    "\n",
    "    # 10% for testing\n",
    "    testProportion = 0.1\n",
    "    trainNb = int(X.shape[0] * (1 - testProportion))\n",
    "    testNb = X.shape[0] - trainNb\n",
    "\n",
    "    print(\"Splitting dataset in X_train({}) and X_test({})...\".format(trainNb, testNb))\n",
    "    # Split train test\n",
    "    X_train = X[:trainNb]\n",
    "    X_test = X[-testNb:]\n",
    "\n",
    "    print(len(X_train))\n",
    "\n",
    "    # Save at hdf5 format\n",
    "    \"Saving dataset in hdf5 format... - this may take a while\"\n",
    "    h5f = h5py.File(\"coins.h5\", 'w')\n",
    "    h5f.create_dataset('X_train', data=X_train)\n",
    "    h5f.create_dataset('X_test', data=X_test)\n",
    "    h5f.create_dataset('roman_coins', data=roman_coins)\n",
    "    h5f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HDF5 dataset \"X_test\": shape (263, 128, 128, 3), type \"<f8\">\n",
      "<class 'h5py._hl.dataset.Dataset'>\n"
     ]
    }
   ],
   "source": [
    "file = Path(\"coins.h5\")\n",
    "if not file.is_file():\n",
    "    save()\n",
    "X_test = h5py.File('coins.h5','r')['X_test']\n",
    "X_train = h5py.File('coins.h5','r')['X_train']\n",
    "print(X_test)\n",
    "print(type(X_test))\n",
    "coins_index = h5py.File('coins.h5','r')['roman_coins']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "roman_coins = []\n",
    "for i in coins_index:\n",
    "    roman_coins += [i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = []\n",
    "y_test = []\n",
    "test_limit = len(X_train)\n",
    "for i, data in enumerate(X_train):\n",
    "    if i in roman_coins:\n",
    "        y_train += [1]\n",
    "    else:\n",
    "        y_train += [0]\n",
    "\n",
    "for i, data in enumerate(X_test):\n",
    "    if i + test_limit in roman_coins:\n",
    "        y_test += [1]\n",
    "    else:\n",
    "        y_test += [0]\n",
    "\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2366, 128, 128, 3)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data\n",
      "0/2366\n",
      "100/2366\n",
      "200/2366\n",
      "300/2366\n",
      "400/2366\n",
      "500/2366\n",
      "600/2366\n",
      "700/2366\n",
      "800/2366\n",
      "900/2366\n",
      "1000/2366\n",
      "1100/2366\n",
      "1200/2366\n",
      "1300/2366\n",
      "1400/2366\n",
      "1500/2366\n",
      "1600/2366\n",
      "1700/2366\n",
      "1800/2366\n",
      "1900/2366\n",
      "2000/2366\n",
      "2100/2366\n",
      "2200/2366\n",
      "2300/2366\n",
      "Testing data\n",
      "0/2366\n",
      "100/2366\n",
      "200/2366\n"
     ]
    }
   ],
   "source": [
    "x_shape = X_train.shape\n",
    "x_train = np.ndarray(shape=X_train.shape, dtype=float)\n",
    "x_test = np.ndarray(shape=X_test.shape, dtype=float)\n",
    "print(\"Training data\")\n",
    "for i, data in enumerate(X_train):\n",
    "    if i % 100 == 0:\n",
    "        print(\"{}/{}\".format(i, len(X_train)))\n",
    "    x_train[i] = data\n",
    "print(\"Testing data\")\n",
    "for i, data in enumerate(X_test):\n",
    "    if i % 100 == 0:\n",
    "        print(\"{}/{}\".format(i, len(X_train)))\n",
    "    x_test[i] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.ravel(x_train)\n",
    "x_test = np.ravel(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define the characteristics of the input image \n",
    "height = 128\n",
    "width = 128\n",
    "channels = 3\n",
    "n_inputs = x_train.shape[0]\n",
    "# n_inputs = height * width \n",
    "\n",
    "conv1_fmaps = 16       #32\n",
    "conv1_ksize = 3\n",
    "conv1_stride = 1\n",
    "conv1_pad = \"SAME\"\n",
    "\n",
    "conv2_fmaps = 24     #64\n",
    "conv2_ksize = 3\n",
    "conv2_stride = 2\n",
    "conv2_pad = \"SAME\"\n",
    "\n",
    "pool3_fmaps = conv2_fmaps\n",
    "\n",
    "n_fc1 = 64\n",
    "n_outputs = 10\n",
    "\n",
    "with tf.name_scope(\"inputs\"):\n",
    "    # Variable X is passed as a vector\n",
    "    X = tf.placeholder(tf.float32, shape=[None, n_inputs], name=\"X\")\n",
    "    # It is reshaped to the tensor according to image size an channels\n",
    "    X_reshaped = tf.reshape(X, shape=[-1, height, width, channels])\n",
    "    # Class of each MNIST image\n",
    "    y = tf.placeholder(tf.int32, shape=[None], name=\"y\")\n",
    "    \n",
    "# The first layer is defined. Notice that the tensorflow function used\n",
    "# is tf.layers.conv2d(). Also the parameters are those previously defined.\n",
    "\n",
    "\n",
    "conv1 = tf.layers.conv2d(X_reshaped, filters=conv1_fmaps, kernel_size=conv1_ksize,\n",
    "                         strides=conv1_stride, padding=conv1_pad,\n",
    "                         activation=tf.nn.relu, name=\"conv1\")\n",
    "\n",
    "# The second layer is defined. Notice that the input of this layer is the output\n",
    "# of the previous layer. You can check that conv1 has size 28x28 and conv2 has\n",
    "# size 14x14 (Try to find out why)\n",
    "\n",
    "conv2 = tf.layers.conv2d(conv1, filters=conv2_fmaps, kernel_size=conv2_ksize,\n",
    "                         strides=conv2_stride, padding=conv2_pad,\n",
    "                         activation=tf.nn.relu, name=\"conv2\")\n",
    "\n",
    "with tf.name_scope(\"pool3\"):\n",
    "    pool3 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"VALID\")\n",
    "    pool3_flat = tf.reshape(pool3, shape=[-1, pool3_fmaps * 7 * 7])\n",
    "\n",
    "# This is the full layer. From previous classes we already know function tf.layers.dense()\n",
    "# used to define full layers. \n",
    "# The number of input and output units is the same (pool3_fmaps * 7 * 7)\n",
    "with tf.name_scope(\"fc1\"):\n",
    "    fc1 = tf.layers.dense(pool3_flat, n_fc1, activation=tf.nn.relu, name=\"fc1\")\n",
    "\n",
    "# This is the output layer where the network produces a classification for each class\n",
    "# The classification is used using the function softmax that we have studied in the previous lab\n",
    "with tf.name_scope(\"output\"):\n",
    "    logits = tf.layers.dense(fc1, n_outputs, name=\"output\")\n",
    "    Y_proba = tf.nn.softmax(logits, name=\"Y_proba\")\n",
    "\n",
    "# The loss function and optimizers are defined as in previous labs.    \n",
    "with tf.name_scope(\"train\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y)\n",
    "    loss = tf.reduce_mean(xentropy)\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "# We define two functions to evaluate the quality of the network as classifier\n",
    "# Correct computes, for a batch of observations, how many were correctly classified.\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "# We define a saver \n",
    "with tf.name_scope(\"init_and_save\"):\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_batch(num, data):\n",
    "\n",
    "    '''\n",
    "    Return a total of `num` random samples and labels. \n",
    "    '''\n",
    "    idx = np.arange(0 , len(data))\n",
    "    print(idx)\n",
    "    np.random.shuffle(idx)\n",
    "    idx = idx[:num]\n",
    "    \n",
    "    data_shuffle = data[idx,:]\n",
    "    print(_)\n",
    "  \n",
    "    return data_shuffle, _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "config = tf.ConfigProto(device_count = {'GPU': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'float' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-2ce8687a39c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mX_random\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midxs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mY_random\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midxs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPERIOD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mbatch_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_random\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mbatch_Y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY_random\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'float' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "EPOCH = 100\n",
    "BATCH_SIZE = 128\n",
    "TRAIN_DATASIZE,_,_,_ = x_shape\n",
    "PERIOD = TRAIN_DATASIZE/BATCH_SIZE #Number of iterations for each epoch\n",
    "\n",
    "for e in range(EPOCH):\n",
    "    idxs = np.random.permutation(TRAIN_DATASIZE) #shuffled ordering\n",
    "    X_random = x_train[idxs]\n",
    "    Y_random = y_train[idxs]\n",
    "    for i in range(len(X_train) // BATCH_SIZE):\n",
    "        batch_X = X_random[i * BATCH_SIZE:(i+1) * BATCH_SIZE]\n",
    "        batch_Y = Y_random[i * BATCH_SIZE:(i+1) * BATCH_SIZE]\n",
    "        sess.run(train,feed_dict = {X: batch_X, Y:batch_Y})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-819038935d1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0macc_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-ced91e91f855>\u001b[0m in \u001b[0;36mnext_batch\u001b[0;34m(num, data)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mReturn\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtotal\u001b[0m \u001b[0mof\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mrandom\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     '''\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_epochs = 3\n",
    "IMAGE_SIZE = 128\n",
    "NUM_CHANNELS = 1\n",
    "BATCH_SIZE = 100\n",
    "test_size = 10000\n",
    "\n",
    "with  tf.Session(config=config) as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(len(X_train) // BATCH_SIZE):\n",
    "            X_batch, y_batch = next_batch(BATCH_SIZE, x_train)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "    print(acc_train)\n",
    "        \n",
    "#         correct_pred = 0\n",
    "#         for iteration in range(mnist.test.num_examples // BATCH_SIZE):\n",
    "#             X_batch, y_batch = mnist.test.next_batch(BATCH_SIZE)\n",
    "#             correct_pred += np.sum(correct.eval(feed_dict={X: X_batch, y: y_batch}))\n",
    "#             #print(correct_pred)\n",
    "            \n",
    "#         acc_test = (correct_pred / float(test_size))\n",
    "        #acc_test = accuracy.eval(feed_dict={X: mnist.test.images, y: mnist.test.labels})\n",
    "#         print(epoch, \"Train accuracy:\", acc_train, \"Test accuracy:\", acc_test)\n",
    "\n",
    "        #save_path = saver.save(sess, \"./my_mnist_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
