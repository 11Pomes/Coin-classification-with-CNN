{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coin Classification using Convolutional Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import imutils\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "import h5py\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "import matplotlib as mp\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset creation.\n",
    "\n",
    "Functions flip, rotate are for expanding coins dataset with modifying coins image by flipping and rotating. Folders Flipped, Rotated should be __existing in the same file system with this project__.\n",
    "\n",
    "Functions names and short explanation: \n",
    "- Images : 2629 coins images. 180 of 2629 are coins from Roman times.\n",
    "- Flipped: 10516 with flipped coins images. - For each coin image add 4 images of flipped coin versions.\n",
    "- Rotated: 21032 with rotated coins images. - For each coin image add 8 images of rotated coin versions.\n",
    "\n",
    "Expanded dataset consist of 21032 images number representation which is fallowing this order: 2629 (Base coin images) \\* 4 (Each original coin image is flipped 4 times) * 2 (Each flipped image is rotated two times).\n",
    "\n",
    "Images which starts with substring __'class'__ are classified as a Roman coins.\n",
    "\n",
    "All images are resized to 64x64 pixels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imageSize = 64\n",
    "roman_index = []\n",
    "\n",
    "def flip():\n",
    "    global roman_index\n",
    "    files = [file for file in os.listdir(\"Images\") if file.endswith(\".png\") or file.endswith(\".jpg\")]\n",
    "    \n",
    "    for i, file in enumerate(files):\n",
    "        print(\"Flipping file {}/{}\".format(i,len(files)))\n",
    "\n",
    "        img = cv2.imread(\"Images/\"+file,-1)\n",
    "        rflip = cv2.flip(img,1)\n",
    "        vflip = cv2.flip(img,0)\n",
    "        rvflip = cv2.flip(rflip,0)\n",
    "\n",
    "        cv2.imwrite(\"Flipped/\"+file[:-4]+\"_.png\",img)\n",
    "        cv2.imwrite(\"Flipped/\"+file[:-4]+\"_r.png\",rflip)\n",
    "        cv2.imwrite(\"Flipped/\"+file[:-4]+\"_v.png\",vflip)\n",
    "        cv2.imwrite(\"Flipped/\"+file[:-4]+\"_rv.png\",rvflip)\n",
    "# Generates 2 rotations of the images\n",
    "def rotate():\n",
    "    global roman_index\n",
    "    files = [file for file in os.listdir(\"Flipped\") if file.endswith(\".png\") or file.endswith(\".jpg\")]\n",
    "\n",
    "    # 2 rotations * 4 flips = 8 possibilites\n",
    "    for index, file in enumerate(files):\n",
    "        print(\"Rotating file {}/{}\".format(index,len(files))) \n",
    "        \n",
    "        img = cv2.imread(\"Flipped/\"+file,-1)\n",
    "        cv2.imwrite(\"Rotated/\"+file[:-4]+\"_0.png\",img)\n",
    "        img = np.rot90(img)\n",
    "        cv2.imwrite(\"Rotated/\"+file[:-4]+\"_90.png\",img)\n",
    "\n",
    "# Save h5 dataset\n",
    "def save():\n",
    "    global roman_index\n",
    "    #Load, normalize and reshape\n",
    "    files = os.listdir(\"Rotated/\")\n",
    "    files = [file for file in files if file.endswith(\".png\")]\n",
    "\n",
    "    shuffle(files)\n",
    "    X = []\n",
    "    for i, file in enumerate(files):\n",
    "        print(\"Adding file {}/{}\".format(i,len(files)))\n",
    "        \n",
    "        if(file.startswith('class')):\n",
    "            roman_index += [i]\n",
    "            \n",
    "        img = cv2.imread(\"Rotated/\"+file, -1)\n",
    "\n",
    "        # Convert grayscale to RGB\n",
    "        if len(img.shape) == 2:\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "        # Resize to desired size and normalize\n",
    "        img = cv2.resize(img,(imageSize,imageSize))\n",
    "        img = img.astype(float)/255.\n",
    "        X.append(img)\n",
    "\n",
    "    # Convert to numpy array\n",
    "    X = np.array(X)\n",
    "\n",
    "    # 40% for testing\n",
    "    testProportion = 0.4\n",
    "    trainNb = int(X.shape[0]*(1-testProportion))\n",
    "    testNb = X.shape[0] - trainNb\n",
    "\n",
    "    print(\"Splitting dataset in X_train({}) and X_test({})...\".format(trainNb,testNb))\n",
    "    # Split train test\n",
    "    X_train = X[:trainNb]\n",
    "    X_test = X[-testNb:]\n",
    "\n",
    "    #Save at hdf5 format\n",
    "    print(\"Saving dataset in hdf5 format... - this may take a while\")\n",
    "    h5f = h5py.File(\"coins.h5\", 'w')\n",
    "    h5f.create_dataset('X_train', data=X_train)\n",
    "    h5f.create_dataset('X_test', data=X_test)\n",
    "    h5f.create_dataset('roman_coins', data=roman_index)\n",
    "    h5f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset is saved as h5 extension. \n",
    "Full generated dataset name: \"coins.h5\".\n",
    "\n",
    "If coins.h5 file not exist in the current project file system, functions for dataset generation(flip, rotate and save) are executed. Otherwise all required components are extracted from generated dataset.\n",
    "\n",
    "### Required components:\n",
    "- X_train : Coins images representation in RGB(Red, Green, Blue) number notation for each image pixel - used for training Convolution Network.\n",
    "- X_test : Coins images representation in RGB(Red, Green, Blue) number notation for each image pixel - used for testing Convolution Network performance.\n",
    "- roman_coins: A list of indexes of Roman coins - used for classification of Roman and non Roman coins.\n",
    "\n",
    "Training and testing data was splitted into two parts: Training data consist of 12619 units of coins representation data and testing data has 8413 units of coins representation data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = Path(\"coins.h5\")\n",
    "if not file.is_file():\n",
    "    print(\"This script creates 21208k images from the base 2652\")\n",
    "    print (\"Make sure the folders Rotated and Flipped exist\")\n",
    "    flip()\n",
    "    rotate()\n",
    "    save()\n",
    "X_train = h5py.File('coins.h5','r')['X_train']\n",
    "X_test = h5py.File('coins.h5','r')['X_test']\n",
    "coins_index = h5py.File('coins.h5','r')['roman_coins']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing.\n",
    "\n",
    "Since features training and testing data is already available from h5 dataset, it is necessary convert data to Python friendly data structure - numpy array list. \n",
    "<br>\n",
    "For classifcation part, it is needed to extract values from h5 dataset with indexes of Roman coins and preprocess this data for all available coins. \n",
    "\n",
    "### Classification\n",
    "A new list witch contains an index of Roman coins is created to extract and save values from h5 dataset to a python based structure. When this step is done, a labels variables y_train and y_test is created which stores each picture classification value:\n",
    "- 0 : Representation of non Roman coin.\n",
    "- 1 : Representation of Roman coin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roman_coins = []\n",
    "for i in coins_index:\n",
    "    roman_coins += [i]\n",
    "y_train = []\n",
    "y_test = []\n",
    "test_limit = len(X_train)\n",
    "for i, data in enumerate(X_train):\n",
    "    if i in roman_coins:\n",
    "        y_train += [1]\n",
    "    else:\n",
    "        y_train += [0]\n",
    "\n",
    "for i, data in enumerate(X_test):\n",
    "    if i + test_limit in roman_coins:\n",
    "        y_test += [1]\n",
    "    else:\n",
    "        y_test += [0]\n",
    "\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features extraction from h5 dataset\n",
    "All images are traversed and stored to the Python data structure for Convolution Network processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train = np.ndarray(shape=X_train.shape, dtype=float)\n",
    "x_test = np.ndarray(shape=X_test.shape, dtype=float)\n",
    "for i, data in enumerate(X_train):\n",
    "    x_train[i] = data\n",
    "for i, data in enumerate(X_test):\n",
    "    x_test[i] = data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution Network\n",
    "Parameters short explanation for Convolution Network:\n",
    "- __Height and Width__ - Image height and weight. Our images are made of 64*64 pixels, so as a height and width numbers are the same.\n",
    "- __Channels__ - Represents image color set. In our project images consist of combination of three colors: red, green, blue(RGB). In this case channels value is 3.\n",
    "- __n_inputs__ - Number of inputs which are calculated by multiplying height, width and channel number. In our project inputs value is 12288(64\\*64*3).\n",
    "- __ntrain__ - Number of all instances of training set (18928).\n",
    "- __ntest__ - Number of all instances of testing set (2104).\n",
    "- __conv1_fmaps, conv2_fmaps__ - Feature map for the first(conv1_fmaps) and second(conv2_fmaps) layers. The feature map is the output of one filter applied to the previous layer. A given filter is drawn across the entire previous layer, moved one pixel at a time. Each position results in an activation of the neuron and the output is collected in the feature map.\n",
    "- __conv1_ksize, conv2_ksize__ - Kernel size for first and second layers. Kernels are used for feature detection.\n",
    "- __conv1_stride, conv2_stride__ - The amount of shifts.\n",
    "- __conv1_pad, conv2_pad__ - Padding parameters. Parameter value is equal 'Same' which is Trying to pad evenly left and right, but if the amount of columns to be added is odd, it will add the extra column to the right\n",
    "- __pool3_fmaps__ - Pooling feature map.\n",
    "- __n_fc1__ - Fully connected layer. Fully connected layer looks at what high level features most strongly correlate to a particular class and has particular weights so that when you compute the products between the weights and the previous layer, you get the correct probabilities for the different classes.\n",
    "- __n_outputs__ - Number of classifiers. Outputs value equals to 2, because there are two possible prediction outputs: 0(Non Roman coin) and 1(Roman coin). \n",
    "\n",
    "Convolution Network has 2 convolution layers: __conv1 and conv2__. These layers create a convolution kernels that are convolved with the layer input to produce a tensor of outputs.\n",
    "<br>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define the characteristics of the input image \n",
    "height = 64\n",
    "width = 64\n",
    "channels = 3\n",
    "n_inputs = height * width * 3\n",
    "ntrain = len(X_train)\n",
    "ntest  = len(X_test)\n",
    "\n",
    "# We define the parameters of the layers according to \n",
    "# description previously presented\n",
    "\n",
    "conv1_fmaps = 16       #32\n",
    "conv1_ksize = 3\n",
    "conv1_stride = 1\n",
    "conv1_pad = \"SAME\"\n",
    "\n",
    "conv2_fmaps = 24     #64\n",
    "conv2_ksize = 3\n",
    "conv2_stride = 2\n",
    "conv2_pad = \"SAME\"\n",
    "\n",
    "pool3_fmaps = conv2_fmaps\n",
    "\n",
    "n_fc1 = 64\n",
    "n_outputs = 2\n",
    "\n",
    "\n",
    "with tf.name_scope(\"inputs\"):\n",
    "    # Variable X is passed as a vector\n",
    "    X = tf.placeholder(tf.float32, shape=[None, n_inputs], name=\"X\")\n",
    "    # It is reshaped to the tensor according to image size an channels\n",
    "    X_reshaped = tf.reshape(X, shape=[-1, height, width, channels])\n",
    "    # Class of each MNIST image\n",
    "    y = tf.placeholder(tf.int32, shape=[None], name=\"y\")\n",
    "\n",
    "# The first layer is defined. Notice that the tensorflow function used\n",
    "# is tf.layers.conv2d(). Also the parameters are those previously defined.\n",
    "\n",
    "\n",
    "conv1 = tf.layers.conv2d(X_reshaped, filters=conv1_fmaps, kernel_size=conv1_ksize,\n",
    "                         strides=conv1_stride, padding=conv1_pad,\n",
    "                         activation=tf.nn.relu, name=\"conv1\")\n",
    "\n",
    "# The second layer is defined. Notice that the input of this layer is the output\n",
    "# of the previous layer. You can check that conv1 has size 28x28 and conv2 has\n",
    "# size 14x14 (Try to find out why)\n",
    "\n",
    "conv2 = tf.layers.conv2d(conv1, filters=conv2_fmaps, kernel_size=conv2_ksize,\n",
    "                         strides=conv2_stride, padding=conv2_pad,\n",
    "                         activation=tf.nn.relu, name=\"conv2\")\n",
    "\n",
    "# The maxpool layer is defined. Notice that the  tf.nn.max_pool() is used to define the layer.\n",
    "# Also, maxpool is applied to each of the conv2_fmaps filters, and since the inputs have size\n",
    "# 14x14, Stride=2 and Padding=Valid, after applying maxpool we have pool3_fmaps filters\n",
    "# of size (7x7). That is the reason while the output is reshaped (flattened) to (pool3_fmaps * 7 * 7)\n",
    "\n",
    "with tf.name_scope(\"pool3\"):\n",
    "    pool3 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"VALID\")\n",
    "    pool3_flat = tf.reshape(pool3, shape=[-1, pool3_fmaps * 16 * 16])\n",
    "\n",
    "# This is the full layer. From previous classes we already know function tf.layers.dense()\n",
    "# used to define full layers. \n",
    "# The number of input and output units is the same (pool3_fmaps * 7 * 7)\n",
    "with tf.name_scope(\"fc1\"):\n",
    "    fc1 = tf.layers.dense(pool3_flat, n_fc1, activation=tf.nn.relu, name=\"fc1\")\n",
    "\n",
    "# This is the output layer where the network produces a classification for each class\n",
    "# The classification is used using the function softmax that we have studied in the previous lab\n",
    "with tf.name_scope(\"output\"):\n",
    "    logits = tf.layers.dense(fc1, n_outputs, name=\"output\")\n",
    "    Y_proba = tf.nn.softmax(logits, name=\"Y_proba\")\n",
    "\n",
    "# The loss function and optimizers are defined as in previous labs.    \n",
    "with tf.name_scope(\"train\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y)\n",
    "    loss = tf.reduce_mean(xentropy)\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "# We define two functions to evaluate the quality of the network as classifier\n",
    "# Correct computes, for a batch of observations, how many were correctly classified.\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "# We define a saver \n",
    "with tf.name_scope(\"init_and_save\"):\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warning!!!\n",
    "n_epochs = 10 takes around ~9 min to compute and more computation resources. It is possible to set n_epochs value to the 1 - it still produce great results of accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10 #1\n",
    "IMAGE_SIZE = 64\n",
    "NUM_CHANNELS = 3\n",
    "BATCH_SIZE = 100\n",
    "test_size = ntest\n",
    "\n",
    "\n",
    "\n",
    "def getActivations(layer,stimuli):\n",
    "    units = sess.run(layer,feed_dict={X:np.reshape(stimuli,[1,n_inputs],order='F')})\n",
    "    plotNNFilter(units)\n",
    "\n",
    "\n",
    "def plotNNFilter(units):\n",
    "    filters = units.shape[3]\n",
    "    plt.figure(2, figsize=(60,60))\n",
    "    n_columns = 4\n",
    "    n_rows = math.ceil(filters / n_columns) + 1\n",
    "    for i in range(filters):\n",
    "        plt.subplot(n_rows, n_columns, i+1)\n",
    "        plt.title('Filter ' + str(i))\n",
    "        plt.imshow(units[0,:,:,i])\n",
    "\n",
    "\n",
    "config = tf.ConfigProto(device_count = {'GPU': 0})\n",
    "with  tf.Session(config=config) as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(ntrain // BATCH_SIZE):\n",
    "            randidx = np.random.randint(ntrain, size=BATCH_SIZE)\n",
    "            X_batch = x_train[randidx, :]\n",
    "            y_batch = y_train[randidx]  \n",
    "            X_batch = np.reshape(X_batch, (-1, n_inputs))\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "\n",
    "\n",
    "        correct_pred = 0\n",
    "        for iteration in range(ntest // BATCH_SIZE):\n",
    "            randidx = np.random.randint(ntest, size=BATCH_SIZE)\n",
    "            X_batch = x_test[randidx, :]\n",
    "            y_batch = y_test[randidx]  \n",
    "            X_batch = np.reshape(X_batch, (-1, n_inputs))\n",
    "#             correct_pred += np.sum(correct.eval(feed_dict={X: X_batch, y: y_batch}))\n",
    "#             print(correct_pred / float(test_size))\n",
    "        acc_test = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        print(epoch + 1, \"Train accuracy:\", acc_train, \"Test accuracy:\", acc_test)\n",
    "        imageToUse = x_test[120]\n",
    "        \n",
    "    plt.imshow(imageToUse)\n",
    "    getActivations(conv1,imageToUse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
